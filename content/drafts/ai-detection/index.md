---
title: Do AI detection tools work?
description: --
order: 159
date: '2025-11-29'
popular: true
tags: ["ai"]
---

The runaway success of generative AI has spawned a [billion-dollar](https://www.novaoneadvisor.com/report/ai-detector-market) sub-industry of "AI detection tools": tools that purport to tell you if a piece of text was written by a human being or generated by an AI tool like ChatGPT. How could that possibly work? Does it work?

### Why AI detection tools cannot be reliable

My initial reaction when I heard about these tools was "there's no way that could ever work". I think that initial reaction is broadly correct, because the core idea of AI detection tools - that there is an intrinsic difference between human-generated writing and AI-generated writing - is just fundamentally mistaken.

Large language models learn from huge training sets of human-written text. They learn to generate text that is as close as possible to the text in their training data. It's this data that determines the basic "voice" of an AI model, not anything about the fact that it's an AI model. A model trained on Shakespeare will sound like Shakespeare, and so on. You could train a thousand different models on a thousand different training sets without finding a common "model voice" or signature that all of them share.

**It is impossible to prove that a piece of text was written by AI.** Anything generated by a language model is _by definition_ the kind of thing that could have been generated by a human.

### Why AI detection tools might work anyway

But of course it's possible to tell when something was written by AI! When I read Twitter replies, the obviously-LLM-generated ones stick out like a sore thumb. I wrote about this in [_Why does AI slop feel so bad to read?_](/on-slop). How can this be possible, when it's impossible to prove that something was written by AI?

Part of the answer here might just be that **current-generation AI models have a really annoying "house style", and any humans writing in the same style are annoying in the same way**. When I read the first sentence of a blog post and think "oh, this is AI slop, no need to keep reading", I don't actually care whether it's AI or not. If it's a human, they're still writing in the style of AI slop, and I still don't want to read the rest of the post.

However, I think there's more going on here. Claude does kind of sound like ChatGPT a lot of the time, even though they're different models trained in different ways on (at least partially) different data. I think the optimistic case for AI detection tooling goes something like this:

- [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback) and instruction/safety tuning pushes all strong LLMs towards the same kind of tone and style
- That tone and style can be automatically detected by training a classifier model
- Sure, it's possible for technically-sophisticated users to use [abliterated](https://huggingface.co/blog/mlabonne/abliteration) LLMs or less-safety-tuned open models, but 99% of users will just be using ChatGPT or Claude (particularly if they're lazy enough to cheat on their essays in the first place)
- Thus a fairly simple "ChatGPT/Claude/Gemini prose style detector" can get you 90% of the way towards detecting most people using LLMs to write their essays

I find this fairly compelling, **so long as these tools are treated as heuristics rather than proofs**. A 90% success rate is surprisingly bad, as illustrated by the classic [Bayes' theorem example](https://tomrocksmaths.com/2021/08/31/bayes-theorem-and-disease-testing/) where testing positive on a 99% accurate test for a rare disease really only represents a ~10% chance that you actually have the disease. If an AI detection tool thinks a piece of writing is AI, you should treat that as "kind of suspicious" instead of conclusive proof.

### How do AI detection tools work?

The best resource I've found for understanding how these tools work is [the EditLens paper](https://arxiv.org/pdf/2510.03154) by Pangram Labs, who make what is plausibly the best AI detection tool out there. The key insight in the EditLens paper is to train a model on text that was _edited_ by AI, not generated from scratch, so the model can learn to predict the granular degree of AI involvement in a particular text. This plausibly gets you a much better classifier than a strict "AI or not" classifier model, because each example teaches the model a numeric _value_ instead of a single bit of information.

[TODO]

### Humanizing tools

Interestingly, there's a sub-sub-industry of "humanizing" tools that aim to convert your AI-generated text into text that will be judged by AI detection tools as "human". Some free AI detection tools are actually sales funnels for these humanizing tools, and will thus deliberately produce a lot of false-positives so users will pay for the humanizing service. For instance, I ran one of my blog posts[^1] through [JustDone](https://justdone.com/ai-detector), which assessed them as 90% AI generated and offered to fix it up for the low, low price of $40 per month.

These tools don't say this outright, but of course the "humanizing" process involves passing your writing through a LLM that's either prompted or fine-tuned to produce less-LLM-sounding content. I find this pretty ironic. There are probably a bunch of students who have been convinced by one of these tools to make their human-written essay LLM-generated, out of (justified) paranoia that a false-positive would get them in real trouble with their school or university.

### False positives and social harm

**It is to almost everyone's advantage to pretend that these tools are better than they are.** The companies that make up the billion-dollar AI detection tool industry obviously want to pretend like they're selling a perfectly reliable tool. University and school administrators want to pretend like they've got the problem under control. People on the internet like dunking on people by posting a screenshot that "proves" they're copying their messages from ChatGPT.

Even the AI labs themselves would like to pretend that AI detection is easy and reliable, since it would relieve them of some of the responsibility they bear for effectively wrecking the education system. OpenAI actually released their own [AI detection tool](https://openai.com/index/new-ai-classifier-for-indicating-ai-written-text/) in January 2023, before [retiring it](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool) six months later due to "its low rate of accuracy".

The real people who suffer from this mirage are the people who are trying to write, but now have to deal with being mistakenly judged for passing AI writing off as their own. I know students who are second-guessing how they write in order to sound "less like AI", or who are recording their keystrokes or taking photos of drafts in order to have some kind of evidence that they can use against false positives.

Because of that, I think the real takeaway here is that 


[^1]: I write every one of these posts with my own disgusting human fingers.