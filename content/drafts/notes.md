https://www.crassh.cam.ac.uk/wp-content/uploads/2025/01/Freeman-1970.pdf

On screwing up

---
glue work as martyrdom
how to effectively do glue work
recognizing when you do and don't have the space for it
getting credit for glue
when NOT to do glue work (when you're throwing your body to patch up things that are epxected to fail)

You probably shouldn't be doing [glue work](https://www.noidea.dog/glue) in large tech companies. I gave some reasons for this in [_Glue work considered harmful_](/glue-work-considered-harmful), but I didn't present an overall theory. Here's one now: **glue work is heroic, and you shouldn't try to be a hero**.

But first, a note on what glue work is and what it isn't. As I'm using the term, glue work is not just work that your company doesn't ask for. It's work that your company doesn't _reward_[^1]. If I make a [side bet](/side-bets) that my team really wants a new suite of unit tests, and I expect my boss to love it, that isn't glue work. That's just work. However, if I write those unit tests knowing that I _won't_ be rewarded for it - in fact, suspecting that I'll be punished for it - that's glue work.

### Should companies just reward glue work?

Should companies reward glue work? I actually don't think so. In my view, rewarding glue work is a bit like rewarding lines of code written. It might correlate with good behavior, but what you really want to reward is _results_.

Some glue work is genuinely useful (for instance, making sure different teams are talking to each other when they need to collaborate on a key project). But some glue work is more "neatness for neatness' sake". Imagine an engineer who really wants to go through past Jira tickets and re-label them in the current scheme, because it makes the historical statistics more consistent. Or imagine an engineer who spends their time chasing people up for individual estimates on tickets, in a team where fine-grained estimation is (perhaps [justifiably](/how-i-estimate-work)) neglected. All kinds of glue work are not created equal.

[^1]: I acknowledge that this is not quite Tanya Reilly's definition of glue work, though Reilly acknowledges that glue work is often unrewarded. In my view, the term has evolved over time to refer to the subset of the original definition that is unrewarded. Or at any rate, that's what I'm interested in writing about in this post, because it's more interesting (if your company rewards glue work, just do it - problem solved!)



Tech companies fund the work they want. If your company really wants to sell software to large enterprises, it will reward engineers who build enterprise features. If your company really wants to avoid embarrassing reliability incidents, it will reward engineers who fix incidents quickly and engineers who do high-profile preventative work.

---

suppose ai really did make it hard to learn new things/cook your brain. it would not follow that SWEs must avoid it
in many other jobs, "doing th ejob makes you worse at th ejob" is common - manual labor injuries/accumulation of damage
it is very pleasant that doing the job lets you learn quickly right now, but that's really just luck - it doesn't _need_ to be true
we're paid to deliver results, not to learn more or become more well-developed


https://www.youtube.com/watch?v=lpuy9RxJmfU
know how to drive the car

https://www.youtube.com/watch?v=s1eqzfXCgXI
AI makes fake legibility cheap (reports, etc - but fake)
"potemkin" reports/dashboards are trivial to generate
legibility "slop"


I am from the finance/politics world, so the on-the-ground perspective of how AI is influencing enterprises is something I think about a lot. 

Working on products people hate

https://matthogg.fyi/a-unified-theory-of-ego-empathy-and-humility-at-work/
in defense of ego

why people hate AI? it's the connection with art
the crypto NFT insistence that 'this is the future of art' about obvious shit
avoiding ARC Raiders because of AI use... well, almost every piece of software you use has had AI contribute, many institutions you interact with have ChatGPT assistance, etc
why do people not boycott that? is it just lack of visibility? or is it that people don't want to be put directly in contact with AI output?

5 min followup
classifier didn't work great - same problem, more or less
what if you heavily constrain sampling?
 What if we started by training a tiny model, then once the loss rate stalls, increase
  the model size and continue, etc? Would that work?

lucky seeds
explains affection for older models


oai syco: https://edition.cnn.com/2025/11/06/us/openai-chatgpt-suicide-lawsuit-invs-vis?utm_source=substack&utm_medium=email 
#keep4o twitter hashtag... man

https://newsletter.posthog.com/p/collaboration-sucks
https://news.ycombinator.com/item?id=45892394
explicitly targeting throughput-not-legibility

interface on top of agentic tooling that gives options to choose from

https://arxiv.org/pdf/2506.02153

of course there's an AI bubble. so what?

how to think clearly about technical topics
concrete
slow
don't have to be smart if you think clearly


https://news.ycombinator.com/user?id=Gigachad
"The most obvious sign of AI slop is mismatched style with the medium"
post - 10 theories of AI slop?

https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf
base rate comparison with normal IT transformations


why would optical prompt compression work?
https://news.ycombinator.com/item?id=45640594


can you improve 5 min training by training a quick classifier for TTC?
maybe?
is it cheating to generate a training set for the classifier outside of the 5 min window
I don't think so, within the rules

popular writing advice I deliberately ignore

auto ai researcher
initially just used ngrams, too quick
happy to churn away
couldn't see MPS in the sandbox, so just used CPU
`codex --sandbox danger-full-access`

promo projects

gpt-5 tone - a new kind of slopß

why are chinese AI models less sycophantic?

why I disagree with https://news.ycombinator.com/item?id=45070512 

https://news.ycombinator.com/user?id=Buttons840

traps for software engineers

the simplest problem models reliably struggle with

instead of you reviewing AI code the AI will review you to make sure you understand its code
dkamm/pr-quiz

the unreasonable effectiveness of prompting


The socratic method and LLMs

You should never be angry in the workplace
ß
 integrations! oxide podcast
 
 a database is a store for global variables
 
 what is inference time compute?
 
