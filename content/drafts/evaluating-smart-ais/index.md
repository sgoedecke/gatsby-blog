---
title: If AIs keep getting smarter, will we be able to tell?
description: --
order: 27
date: '2025-01-10'
---

The big news in the last month (that's two years in AI time) is the fact that the new generation of AI lab models are underwhelming. OpenAI's GPT-4.5 has impressed some people but certainly hasn't caused a giant splash. Anthropic's Sonnet 3.7 has become some people's daily driver, but a lot of engineers I know are sticking with 3.5. You can even tell from the naming: Anthropic and OpenAI are both reluctant to bust out a new major version release.

The [usual suspects](https://garymarcus.substack.com/p/hot-take-gpt-45-is-a-nothing-burger) are heralding this as the end of the line for traditional AI scaling. Whether that's right, it's worth noting that those same people were saying the [same thing](https://garymarcus.substack.com/p/gpt-4s-successes-and-gpt-4s-failures) about GPT-4. Probably the AI labs' own reluctance, and their newfound focus on reasoning models is better evidence that traditional AI scaling is slowing down. But I want to ask a different question. Suppose that base AI models were getting linearly smarter (i.e. that GPT-4.5 really was as far above GPT-4 as GPT-4 was above GPT-3.5). Would we actually be able to tell?

When you're talking to someone who's less smart than you[^1], it's very clear. You can see them failing to follow points you're making, or they just straight up spend time visibly confused and contradicting themselves. But when you're talking to someone smarter than you, it's far from clear (to you) what's going on. You can sometimes feel that you're confused, but that doesn't necessarily mean they're smarter. It could be that they're just talking nonsense.

Talking with AI models is like that. GPT-3.5 was very clearly less smart than most of the humans who talked to it. It was mainly impressive that it was able to carry on a conversation at all. GPT-4 was probably on par with most humans, or at least pretty close. GPT-4.5 is... smarter? If it were, would we know immediately?



[^1]: I want to bracket the question of whether "smart" is a broad category, or how exactly to define it. I'm talking specifically about the way GPT-4 is smarter than GPT-3.5 - even if we can't define exactly how, we know that's a real thing.