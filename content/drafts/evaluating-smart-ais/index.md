---
title: If AIs keep getting smarter, will we be able to tell?
description: --
order: 27
date: '2025-01-10'
---

The big news in the last six months (that's two years in AI time) is the fact that the new generation of AI lab models are underwhelming. OpenAI's GPT-4.5 impressed some people but certainly hasn't caused a giant splash, and has now been discontinued. Anthropic's Sonnet 4 has become some people's daily driver - particularly in Claude Code - but it's only an incremental improvement over 3.7 and even 3.5. The most recent big release is GPT-5, which is a good model, but not dramatically better than the previous best-in-class OpenAI model o3.

In fact, **GPT-5 is not even a scaled-up model.** OpenAI never publishes the size of their models, but looking at their [pricing](https://platform.openai.com/docs/pricing) makes it clear that GPT-5 is on par with or cheaper than GPT-4o and GPT-4.1. Actually large OpenAI models like GPT-4.5 are (or were) [ten to seventy times more expensive](https://www.reddit.com/r/OpenAI/comments/1izpgct/gpt45_has_an_api_price_of_751m_input_and_1501m/). Of course, GPT-5 is not a single model at all, but some combination of fast and slow models. But the average scale must still correlate with price[^1].

Sam Altman, the CEO of OpenAI, has [said](https://x.com/sama/status/1953551377873117369) that GPT-5 was not an attempt to make the smartest possible model, but an attempt to put a frontier model in the hands of as many people as possible. Before the GPT-5 launch, most ChatGPT users were using 4o. GPT-5 is a big improvement over that (despite the people who are [in it for the sycophancy](https://arstechnica.com/information-technology/2025/08/openai-brings-back-gpt-4o-after-user-revolt/) and successfully lobbied to bring 4o back). But we're definitely not seeing the kind of intelligence explosion that many AI enthusiasts were predicting.

The [usual suspects](https://garymarcus.substack.com/p/hot-take-gpt-45-is-a-nothing-burger) are [heralding this](https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming) as the end of the line for traditional AI scaling. Whether that's right, it's worth noting that those same people were saying the [same thing](https://garymarcus.substack.com/p/gpt-4s-successes-and-gpt-4s-failures) about GPT-4. Probably the AI labs' own reluctance, and their newfound focus on reasoning models is better evidence that traditional AI scaling is slowing down. But I want to ask a different question. Suppose that base AI models were getting linearly smarter (i.e. that GPT-4.5 really was as far above GPT-4 as GPT-4 was above GPT-3.5). **Would we actually be able to tell?**

When you're talking to someone who's less smart than you[^2], it's very clear. You can see them failing to follow points you're making, or they just straight up spend time visibly confused and contradicting themselves. But when you're talking to someone smarter than you, it's far from clear (to you) what's going on. You can sometimes feel that you're confused by what they say, but that doesn't necessarily mean they're smarter. It could be that they're just talking nonsense. And smarter people won't confuse you all the time - only when they fail to pitch their communication at your level. 

Talking with AI models is like that. GPT-3.5 was very clearly less smart than most of the humans who talked to it. It was mainly impressive that it was able to carry on a conversation at all. GPT-4 was probably on par with most humans, or at least pretty close. GPT-4.5 is... smarter? If it were, would we know immediately?





[^1]: It's technically possible that OpenAI is pricing this model at a massive loss for PR reasons, but GPT-5 is clearly much _faster_ than GPT-4.5 as well.

[^2]: I want to bracket the question of whether "smart" is a broad category, or how exactly to define it. I'm talking specifically about the way GPT-4 is smarter than GPT-3.5 - even if we can't define exactly how, we know that's a real thing.
